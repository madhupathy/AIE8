{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2beb2d",
   "metadata": {},
   "source": [
    "\n",
    "# LangGraph and LangSmith â€” Agentic RAG Powered by LangChain\n",
    "\n",
    "In this notebook we complete the Session 5 assignment.\n",
    "\n",
    "- ðŸ¤ **Breakout Room #1**\n",
    "  1. Install required libraries\n",
    "  2. Set Environment Variables\n",
    "  3. Creating our Tool Belt\n",
    "  4. Creating Our State\n",
    "  5. Creating and Compiling A Graph!\n",
    "\n",
    "- ðŸ¤ **Breakout Room #2**\n",
    "  1. Evaluating the LangGraph Application with LangSmith\n",
    "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
    "  3. LangGraph for the \"Patterns\" of GenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03dd492",
   "metadata": {},
   "source": [
    "# ðŸ¤ Breakout Room #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db473b",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: LangGraph â€” Building Cyclic Applications with LangChain\n",
    "\n",
    "LangGraph leverages LCEL to build coordinated multi-actor **stateful** apps that support cycles (loops). Cycles let the agent iterate until it has a good answer or hits guardrails you define.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54615a14",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82635747",
   "metadata": {},
   "source": [
    "\n",
    "If needed, install dependencies in your environment (already handled in project setup). In the notebook we import the libraries directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535de8e5",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2: Environment Variables\n",
    "\n",
    "Set OpenAI, Tavily and LangSmith keys and LangSmith tracing project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82840b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "# Set API keys from environment variables or use placeholder values for demo\n",
    "# In production, set these as environment variables or use a .env file\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key-here\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\", \"your-tavily-api-key-here\") \n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\", \"your-langsmith-api-key-here\")\n",
    "\n",
    "# LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE8 - LangGraph - {uuid4().hex[:8]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84b2b6",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3: Creating our Tool Belt\n",
    "\n",
    "We'll use:\n",
    "- **Tavily** web search (via `langchain-tavily`)\n",
    "- **ArXiv** search (via `langchain_community`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTANT: Use the non-deprecated Tavily package\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tavily_tool = TavilySearch(max_results=5)\n",
    "tool_belt = [tavily_tool, ArxivQueryRun()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c390e",
   "metadata": {},
   "source": [
    "\n",
    "### Model\n",
    "\n",
    "We use OpenAI's chat model and **bind** the tool belt using function-calling semantics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ae033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "model = model.bind_tools(tool_belt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb18ad",
   "metadata": {},
   "source": [
    "\n",
    "#### â“ Question #1: How does the model determine which tool to use?\n",
    "\n",
    "**Answer:** The model is provided JSON schemas for the tools via `bind_tools`. On each turn it predicts optional `tool_calls` (function name + JSON args). If `tool_calls` are present in the AI message, our graph routes to the `ToolNode`, which executes the tools and returns results to the agent. The decision of *which* tool and *with what arguments* is learned behavior guided by the tool schemas and the conversation context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a52c8",
   "metadata": {},
   "source": [
    "\n",
    "## Task 4: Putting the State in Stateful\n",
    "\n",
    "We carry a shared `messages` list around the graph so nodes can read/write context as the agent iterates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dba594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57e242",
   "metadata": {},
   "source": [
    "\n",
    "## Task 5: It's Graphing Time!\n",
    "\n",
    "We create two nodes:\n",
    "- `agent`: calls the model\n",
    "- `action`: executes tool calls (if any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def call_model(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node = ToolNode(tool_belt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "uncompiled_graph = StateGraph(AgentState)\n",
    "uncompiled_graph.add_node(\"agent\", call_model)\n",
    "uncompiled_graph.add_node(\"action\", tool_node)\n",
    "uncompiled_graph.set_entry_point(\"agent\")\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if getattr(last_message, \"tool_calls\", None):\n",
    "        return \"action\"\n",
    "    return END\n",
    "\n",
    "# If function returns END we finish; otherwise we go to the named node\n",
    "uncompiled_graph.add_conditional_edges(\"agent\", should_continue)\n",
    "uncompiled_graph.add_edge(\"action\", \"agent\")\n",
    "\n",
    "simple_agent_graph = uncompiled_graph.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aef7a8",
   "metadata": {},
   "source": [
    "\n",
    "#### â“ Question #2: Is there a limit to how many times we can cycle? How could we impose one?\n",
    "\n",
    "**Answer:** There is no inherent limitâ€”LangGraph will keep looping as long as your conditional edges allow it. To cap cycles, add a guard (e.g., a counter or `len(state[\"messages\"])`) and route to `END` once a threshold is reached. You can also add timeouts or a separate \"helpfulness\" gate to exit when the answer is sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37141e8e",
   "metadata": {},
   "source": [
    "\n",
    "## Using Our Graph\n",
    "\n",
    "We can stream updates to see tool calls and iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90000933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"How are technical professionals using AI to improve their work?\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: {node}\")\n",
    "        print(values[\"messages\"])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cca991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Search Arxiv for the 'A Comprehensive Survey of Deep Research' paper, then search each of the authors to find out where they work now using Tavily!\")]} \n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: {node}\")\n",
    "        if node == \"action\" and values[\"messages\"]:\n",
    "            # show which tool executed\n",
    "            try:\n",
    "                print(f\"Tool used: {values['messages'][0].name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(values[\"messages\"])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d68ce9",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ—ï¸ Activity #2 â€” Steps the agent took\n",
    "\n",
    "1. The agent read the user message and produced `tool_calls` indicating which tools to use and with what arguments.  \n",
    "2. The conditional edge detected `tool_calls` and routed to the `action` node.  \n",
    "3. The `ToolNode` executed the tools (ArXiv, then Tavily) and appended their results to state.  \n",
    "4. Control returned to the `agent`, which synthesized the tool outputs into a final answer.  \n",
    "5. With no further `tool_calls`, the conditional edge returned `END`, finishing the run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c074b",
   "metadata": {},
   "source": [
    "# ðŸ¤ Breakout Room #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3ae87",
   "metadata": {},
   "source": [
    "## Part 1: LangSmith Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9c479",
   "metadata": {},
   "source": [
    "### Pre-processing for LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb62dc",
   "metadata": {},
   "source": [
    "Wrap our graph to convert inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_inputs(input_object):\n",
    "    return {\"messages\": [HumanMessage(content=input_object[\"text\"])]}\n",
    "\n",
    "def parse_output(state):\n",
    "    return {\"answer\": state[\"messages\"][-1].content}\n",
    "\n",
    "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output\n",
    "agent_chain_with_formatting.invoke({\"text\": \"What is Deep Research?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fc090",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1: Creating An Evaluation Dataset\n",
    "\n",
    "Create at least 5 examples (we provide 6) related to the cohort use-case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions = [\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Who were the main authors of 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications'?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"Renjun Xu\", \"Jingwen Peng\", \"Xu\", \"Peng\"]},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"When was the 'A Comprehensive Survey of Deep Research' paper published?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"2025\", \"June\"]},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"List two commercial deep research systems mentioned recently.\"},\n",
    "        \"outputs\": {\"must_mention\": [\"OpenAI\", \"Perplexity\"]},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"What four technical dimensions are used to categorize deep research systems?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"reasoning\", \"tools\", \"planning\", \"synthesis\"]},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Name two challenges highlighted for deep research systems.\"},\n",
    "        \"outputs\": {\"must_mention\": [\"accuracy\", \"privacy\"]},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Where do the authors of the 'Deep Research' survey currently work?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"Zhejiang\", \"Liberty Mutual\"]},\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a6184",
   "metadata": {},
   "source": [
    "Add the dataset to LangSmith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ec4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langsmith import Client\n",
    "from uuid import uuid4\n",
    "\n",
    "client = Client()\n",
    "dataset_name = f\"Simple Search Agent - Evaluation Dataset - {uuid4().hex[:8]}\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name, description=\"Questions about Deep Research to evaluate the Simple Search Agent.\")\n",
    "client.create_examples(dataset_id=dataset.id, examples=questions)\n",
    "dataset_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204dcd0",
   "metadata": {},
   "source": [
    "### Task 2: Adding Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33783dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "print(CORRECTNESS_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab66829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openevals.llm import create_llm_as_judge\n",
    "\n",
    "correctness_evaluator = create_llm_as_judge(\n",
    "    prompt=CORRECTNESS_PROMPT,\n",
    "    model=\"openai:o3-mini\",   # tune as needed\n",
    "    feedback_key=\"correctness\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f74266",
   "metadata": {},
   "source": [
    "\n",
    "Custom **must_mention** evaluator (improved for normalization and partial credit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88755c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def _normalize(t: str) -> str:\n",
    "    return re.sub(r\"\\W+\", \" \", (t or \"\").lower()).strip()\n",
    "\n",
    "def must_mention(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\n",
    "    required = reference_outputs.get(\"must_mention\") or []\n",
    "    out = _normalize(outputs.get(\"answer\", \"\"))\n",
    "    hits = 0\n",
    "    for phrase in required:\n",
    "        if _normalize(phrase) in out:\n",
    "            hits += 1\n",
    "    return hits / max(1, len(required))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d5ea8d",
   "metadata": {},
   "source": [
    "### Task 3: Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = client.evaluate(\n",
    "    agent_chain_with_formatting,\n",
    "    data=dataset.name,\n",
    "    evaluators=[correctness_evaluator, must_mention],\n",
    "    experiment_prefix=\"simple_agent, baseline\",\n",
    "    description=\"Testing the baseline system.\",\n",
    "    max_concurrency=4,\n",
    ")\n",
    "\n",
    "print(\"If running in a traced environment, open LangSmith to view comparison for the latest experiment.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5811c",
   "metadata": {},
   "source": [
    "## Part 2: LangGraph with Helpfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8662c4e",
   "metadata": {},
   "source": [
    "\n",
    "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
    "\n",
    "We add a conditional that either (a) executes tools, (b) ends if the answer is helpful, or (c) loops back for another refinement. We also impose a hard cap on turns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d813033",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** We instantiate a new `StateGraph(AgentState)` and add two nodes: `agent` (calls the LLM) and `action` (executes tool calls). We'll add a helpfulness gate next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efcf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_with_helpfulness_check = StateGraph(AgentState)\n",
    "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
    "graph_with_helpfulness_check.add_node(\"action\", tool_node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a9950",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** Set `\"agent\"` as the entry point so every run starts with the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad92df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_with_helpfulness_check.set_entry_point(\"agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3f740",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** `tool_call_or_helpful` routes to `action` if the model emitted `tool_calls`. Otherwise it runs a **helpfulness** check comparing the initial query and current final response. If helpful â†’ end; else â†’ continue. A hard cap prevents infinite loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def tool_call_or_helpful(state: AgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if getattr(last_message, \"tool_calls\", None):\n",
    "        return \"action\"\n",
    "\n",
    "    # hard loop cap\n",
    "    if len(state[\"messages\"]) > 10:\n",
    "        return \"end\"  # use mapping below\n",
    "\n",
    "    initial_query = state[\"messages\"][0]\n",
    "    final_response = state[\"messages\"][-1]\n",
    "\n",
    "    prompt_template = \"\"\"Given an initial query and a final response, determine if the final response is extremely helpful or not.\n",
    "Reply with 'Y' if helpful, 'N' if not.\n",
    "\n",
    "Initial Query:\n",
    "{initial_query}\n",
    "\n",
    "Final Response:\n",
    "{final_response}\n",
    "\"\"\"\n",
    "    helpfulness_prompt = PromptTemplate.from_template(prompt_template)\n",
    "    helpfulness_check_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "    helpfulness_chain = helpfulness_prompt | helpfulness_check_model | StrOutputParser()\n",
    "    helpfulness_response = helpfulness_chain.invoke({\n",
    "        \"initial_query\": initial_query.content,\n",
    "        \"final_response\": final_response.content\n",
    "    })\n",
    "    return \"end\" if \"Y\" in helpfulness_response else \"continue\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d9982",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** Connect conditional outcomes: `\"action\"` â†’ `action`, `\"continue\"` â†’ loop back to `agent`, `\"end\"` â†’ finish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "graph_with_helpfulness_check.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tool_call_or_helpful,\n",
    "    {\n",
    "        \"continue\": \"agent\",\n",
    "        \"action\": \"action\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13af36",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** Tool results always flow back to `agent` so the model can read them and decide next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49740f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f246e",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** Compile to a runnable graph and test with a prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"What are Deep Research Agents?\")]}\n",
    "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: {node}\")\n",
    "        print(values[\"messages\"])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf8424",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: LangGraph for the \"Patterns\" of GenAI\n",
    "\n",
    "### Task 4: Helpfulness Check of GenAI Pattern Descriptions\n",
    "Ask the system about the 3 main patterns: Context Engineering, Fine-tuning, and Agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36172055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patterns = [\"Context Engineering\", \"Fine-tuning\", \"LLM-based agents\"]\n",
    "\n",
    "for pattern in patterns:\n",
    "    q = f\"What is {pattern} and when did it break onto the scene?\"\n",
    "    inputs = {\"messages\": [HumanMessage(content=q)]}\n",
    "    state = agent_with_helpfulness_check.invoke(inputs)\n",
    "    print(state[\"messages\"][-1].content)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e9184",
   "metadata": {},
   "source": [
    "\n",
    "#### â“ Question #4: How could we improve the `must_mention` metric?\n",
    "\n",
    "Upgraded must_mention evaluator below:\n",
    "\n",
    "What it does better:\n",
    "\n",
    "- Normalizes case/punct/whitespace.\n",
    "- Accepts aliases/synonyms for each required term.\n",
    "- Allows fuzzy matching (>= similarity threshold).\n",
    "- Gives partial credit (0â€“1) with optional weights.\n",
    "- Penalizes extraneous/hallucinated entities if you specify a disallow list.\n",
    "- Optional â€œmust citeâ€ check (URL present) per item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2dd603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "def _norm(t: str) -> str:\n",
    "    return re.sub(r\"\\W+\", \" \", (t or \"\").lower()).strip()\n",
    "\n",
    "def _contains_fuzzy(haystack: str, needle: str, thresh: float = 0.85) -> bool:\n",
    "    \"\"\"Return True if any substring of haystack is similar to needle with ratio>=thresh.\"\"\"\n",
    "    h = haystack\n",
    "    n = needle\n",
    "    if not h or not n:\n",
    "        return False\n",
    "    # Fast path: direct containment\n",
    "    if n in h:\n",
    "        return True\n",
    "    # Fuzzy (approximate) search using SequenceMatcher windows\n",
    "    # Use windows around the needle length\n",
    "    ln = max(4, len(n))  # avoid tiny windows\n",
    "    tokens = h.split()\n",
    "    # Build sliding windows approx the length in tokens\n",
    "    # Simple heuristic: compare against chunks in haystack\n",
    "    for i in range(len(tokens)):\n",
    "        chunk = \" \".join(tokens[i : i + ln])\n",
    "        if SequenceMatcher(None, chunk, n).ratio() >= thresh:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def must_mention(\n",
    "    inputs: dict,\n",
    "    outputs: dict,\n",
    "    reference_outputs: dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Robust must_mention metric with:\n",
    "      - synonyms/aliases per required item\n",
    "      - fuzzy matching\n",
    "      - optional weights\n",
    "      - optional negative penalty list\n",
    "      - optional 'must_cite' URLs presence check\n",
    "    Contract:\n",
    "      reference_outputs may include:\n",
    "        {\n",
    "          \"must_mention\": [\n",
    "            \"Zhejiang University\",\n",
    "            [\"Renjun Xu\", \"R. Xu\"],        # aliases OK as list\n",
    "            {\"term\": \"Liberty Mutual\", \"aliases\": [\"Liberty\"], \"weight\": 2.0, \"must_cite\": True},\n",
    "          ],\n",
    "          \"disallow\": [\"MadeUpCo\", \"FakeLab\"],   # penalize if present\n",
    "          \"fuzzy_threshold\": 0.88                 # override default 0.85\n",
    "        }\n",
    "    Returns a float in [0, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    required: List[Union[str, list, dict]] = reference_outputs.get(\"must_mention\") or []\n",
    "    disallow: List[str] = reference_outputs.get(\"disallow\", [])\n",
    "    thresh: float = float(reference_outputs.get(\"fuzzy_threshold\", 0.85))\n",
    "\n",
    "    out_text = _norm(outputs.get(\"answer\", \"\"))\n",
    "\n",
    "    # Helper to test a single candidate (string) against output\n",
    "    def _hit_one(candidate: str) -> bool:\n",
    "        cand = _norm(candidate)\n",
    "        return _contains_fuzzy(out_text, cand, thresh=thresh)\n",
    "\n",
    "    # Score required terms with optional weights/aliases and optional must_cite\n",
    "    total_weight = 0.0\n",
    "    earned = 0.0\n",
    "\n",
    "    # naive URL presence if must_cite is required for an item\n",
    "    has_url = (\"http://\" in outputs.get(\"answer\", \"\")) or (\"https://\" in outputs.get(\"answer\", \"\"))\n",
    "\n",
    "    for item in required:\n",
    "        weight = 1.0\n",
    "        must_cite = False\n",
    "        aliases: List[str] = []\n",
    "\n",
    "        if isinstance(item, str):\n",
    "            aliases = [item]\n",
    "        elif isinstance(item, list):\n",
    "            # list of aliases\n",
    "            aliases = item\n",
    "        elif isinstance(item, dict):\n",
    "            # {\"term\": \"...\", \"aliases\": [...], \"weight\": 2.0, \"must_cite\": True}\n",
    "            main = item.get(\"term\")\n",
    "            if main:\n",
    "                aliases.append(main)\n",
    "            aliases += item.get(\"aliases\", [])\n",
    "            weight = float(item.get(\"weight\", 1.0))\n",
    "            must_cite = bool(item.get(\"must_cite\", False))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        total_weight += weight\n",
    "\n",
    "        # Mark hit if ANY alias matches\n",
    "        hit = any(_hit_one(a) for a in aliases if a)\n",
    "                                                                  \n",
    "        # If the item requires a citation, we only award if there is a URL\n",
    "        if hit and must_cite and not has_url:\n",
    "            hit = False\n",
    "\n",
    "        if hit:\n",
    "            earned += weight\n",
    "\n",
    "    # Optional penalty for disallowed / hallucinated entities (light penalty)\n",
    "    penalty = 0.0\n",
    "    if disallow:\n",
    "        for bad in disallow:\n",
    "            if _hit_one(bad):\n",
    "                penalty += 0.25   # tune as needed\n",
    "\n",
    "    # Avoid division by zero\n",
    "    base_score = (earned / total_weight) if total_weight > 0 else 0.0\n",
    "    final = max(0.0, min(1.0, base_score - penalty))\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06977d3",
   "metadata": {},
   "source": [
    "How to use it in your dataset quickly, we can keep simple strings as before or use rich items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3fcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Who were the main authors of the Deep Research survey?\"},\n",
    "        \"outputs\": {\n",
    "            \"must_mention\": [\n",
    "                [\"Renjun Xu\", \"R. Xu\"],     # aliases\n",
    "                [\"Jingwen Peng\", \"J. Peng\"]\n",
    "            ],\n",
    "            \"disallow\": [\"Completely Fake Author\"]\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Where do the authors currently work?\"},\n",
    "        \"outputs\": {\n",
    "            \"must_mention\": [\n",
    "                {\"term\":\"Zhejiang University\", \"aliases\":[\"ZJU\"], \"weight\": 2.0},\n",
    "                {\"term\":\"Liberty Mutual\", \"aliases\":[\"Liberty\"], \"must_cite\": True}\n",
    "            ],\n",
    "            \"fuzzy_threshold\": 0.9\n",
    "        },\n",
    "    },\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
