{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2beb2d",
   "metadata": {},
   "source": [
    "\n",
    "# LangGraph and LangSmith ‚Äî Agentic RAG Powered by LangChain\n",
    "\n",
    "In this notebook we complete the Session 5 assignment.\n",
    "\n",
    "- ü§ù **Breakout Room #1**\n",
    "  1. Install required libraries\n",
    "  2. Set Environment Variables\n",
    "  3. Creating our Tool Belt\n",
    "  4. Creating Our State\n",
    "  5. Creating and Compiling A Graph!\n",
    "\n",
    "- ü§ù **Breakout Room #2**\n",
    "  1. Evaluating the LangGraph Application with LangSmith\n",
    "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
    "  3. LangGraph for the \"Patterns\" of GenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03dd492",
   "metadata": {},
   "source": [
    "# ü§ù Breakout Room #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db473b",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: LangGraph ‚Äî Building Cyclic Applications with LangChain\n",
    "\n",
    "LangGraph leverages LCEL to build coordinated multi-actor **stateful** apps that support cycles (loops). Cycles let the agent iterate until it has a good answer or hits guardrails you define.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54615a14",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82635747",
   "metadata": {},
   "source": [
    "\n",
    "If needed, install dependencies in your environment (already handled in project setup). In the notebook we import the libraries directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535de8e5",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2: Environment Variables\n",
    "\n",
    "Set OpenAI, Tavily and LangSmith keys and LangSmith tracing project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82840b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, getpass\n",
    "from uuid import uuid4\n",
    "\n",
    "# Set keys interactively (will not be stored in the repo)\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY: \")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")\n",
    "\n",
    "# LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE8 - LangGraph - {uuid4().hex[:8]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84b2b6",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3: Creating our Tool Belt\n",
    "\n",
    "We'll use:\n",
    "- **Tavily** web search (via `langchain-tavily`)\n",
    "- **ArXiv** search (via `langchain_community`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTANT: Use the non-deprecated Tavily package\n",
    "from langchain_tavily import TavilySearchResults\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "tool_belt = [tavily_tool, ArxivQueryRun()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c390e",
   "metadata": {},
   "source": [
    "\n",
    "### Model\n",
    "\n",
    "We use OpenAI's chat model and **bind** the tool belt using function-calling semantics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ae033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "model = model.bind_tools(tool_belt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb18ad",
   "metadata": {},
   "source": [
    "\n",
    "#### ‚ùì Question #1: How does the model determine which tool to use?\n",
    "\n",
    "**Answer:** The model is provided JSON schemas for the tools via `bind_tools`. On each turn it predicts optional `tool_calls` (function name + JSON args). If `tool_calls` are present in the AI message, our graph routes to the `ToolNode`, which executes the tools and returns results to the agent. The decision of *which* tool and *with what arguments* is learned behavior guided by the tool schemas and the conversation context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a52c8",
   "metadata": {},
   "source": [
    "\n",
    "## Task 4: Putting the State in Stateful\n",
    "\n",
    "We carry a shared `messages` list around the graph so nodes can read/write context as the agent iterates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dba594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57e242",
   "metadata": {},
   "source": [
    "\n",
    "## Task 5: It's Graphing Time!\n",
    "\n",
    "We create two nodes:\n",
    "- `agent`: calls the model\n",
    "- `action`: executes tool calls (if any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def call_model(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node = ToolNode(tool_belt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "uncompiled_graph = StateGraph(AgentState)\n",
    "uncompiled_graph.add_node(\"agent\", call_model)\n",
    "uncompiled_graph.add_node(\"action\", tool_node)\n",
    "uncompiled_graph.set_entry_point(\"agent\")\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if getattr(last_message, \"tool_calls\", None):\n",
    "        return \"action\"\n",
    "    return END\n",
    "\n",
    "# If function returns END we finish; otherwise we go to the named node\n",
    "uncompiled_graph.add_conditional_edges(\"agent\", should_continue)\n",
    "uncompiled_graph.add_edge(\"action\", \"agent\")\n",
    "\n",
    "simple_agent_graph = uncompiled_graph.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aef7a8",
   "metadata": {},
   "source": [
    "\n",
    "#### ‚ùì Question #2: Is there a limit to how many times we can cycle? How could we impose one?\n",
    "\n",
    "**Answer:** There is no inherent limit‚ÄîLangGraph will keep looping as long as your conditional edges allow it. To cap cycles, add a guard (e.g., a counter or `len(state[\"messages\"])`) and route to `END` once a threshold is reached. You can also add timeouts or a separate \"helpfulness\" gate to exit when the answer is sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37141e8e",
   "metadata": {},
   "source": [
    "\n",
    "## Using Our Graph\n",
    "\n",
    "We can stream updates to see tool calls and iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90000933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"How are technical professionals using AI to improve their work?\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: {node}\")\n",
    "        print(values[\"messages\"])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cca991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Search Arxiv for the 'A Comprehensive Survey of Deep Research' paper, then search each of the authors to find out where they work now using Tavily!\")]} \n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: {node}\")\n",
    "        if node == \"action\" and values[\"messages\"]:\n",
    "            # show which tool executed\n",
    "            try:\n",
    "                print(f\"Tool used: {values['messages'][0].name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(values[\"messages\"])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d68ce9",
   "metadata": {},
   "source": [
    "\n",
    "#### üèóÔ∏è Activity #2 ‚Äî Steps the agent took\n",
    "\n",
    "1. The agent read the user message and produced `tool_calls` indicating which tools to use and with what arguments.  \n",
    "2. The conditional edge detected `tool_calls` and routed to the `action` node.  \n",
    "3. The `ToolNode` executed the tools (ArXiv, then Tavily) and appended their results to state.  \n",
    "4. Control returned to the `agent`, which synthesized the tool outputs into a final answer.  \n",
    "5. With no further `tool_calls`, the conditional edge returned `END`, finishing the run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c074b",
   "metadata": {},
   "source": [
    "# ü§ù Breakout Room #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3ae87",
   "metadata": {},
   "source": [
    "## Part 1: LangSmith Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9c479",
   "metadata": {},
   "source": [
    "### Pre-processing for LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb62dc",
   "metadata": {},
   "source": [
    "Wrap our graph to convert inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_inputs(input_object):\n",
    "    return {\"messages\": [HumanMessage(content=input_object[\"text\"])]}\n",
    "\n",
    "def parse_output(state):\n",
    "    return {\"answer\": state[\"messages\"][-1].content}\n",
    "\n",
    "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output\n",
    "agent_chain_with_formatting.invoke({\"text\": \"What is Deep Research?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fc090",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1: Creating An Evaluation Dataset\n",
    "\n",
    "Create at least 5 examples (we provide 6) related to the cohort use-case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions = [\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Who were the main authors of 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications'?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"Renjun Xu\", \"Jingwen Peng\", \"Xu\", \"Peng\"]},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"When was the 'A Comprehensive Survey of Deep Research' paper published?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"2025\", \"June\"]},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"List two commercial deep research systems mentioned recently.\"},\n",
    "        \"outputs\": {\"must_mention\": [\"OpenAI\", \"Perplexity\"]},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"What four technical dimensions are used to categorize deep research systems?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"reasoning\", \"tools\", \"planning\", \"synthesis\"]},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Name two challenges highlighted for deep research systems.\"},\n",
    "        \"outputs\": {\"must_mention\": [\"accuracy\", \"privacy\"]},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Where do the authors of the 'Deep Research' survey currently work?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"Zhejiang\", \"Liberty Mutual\"]},\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a6184",
   "metadata": {},
   "source": [
    "Add the dataset to LangSmith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ec4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langsmith import Client\n",
    "from uuid import uuid4\n",
    "\n",
    "client = Client()\n",
    "dataset_name = f\"Simple Search Agent - Evaluation Dataset - {uuid4().hex[:8]}\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name, description=\"Questions about Deep Research to evaluate the Simple Search Agent.\")\n",
    "client.create_examples(dataset_id=dataset.id, examples=questions)\n",
    "dataset_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204dcd0",
   "metadata": {},
   "source": [
    "### Task 2: Adding Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33783dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "print(CORRECTNESS_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab66829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openevals.llm import create_llm_as_judge\n",
    "\n",
    "correctness_evaluator = create_llm_as_judge(\n",
    "    prompt=CORRECTNESS_PROMPT,\n",
    "    model=\"openai:o3-mini\",   # tune as needed\n",
    "    feedback_key=\"correctness\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f74266",
   "metadata": {},
   "source": [
    "\n",
    "Custom **must_mention** evaluator (improved for normalization and partial credit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88755c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def _normalize(t: str) -> str:\n",
    "    return re.sub(r\"\\W+\", \" \", (t or \"\").lower()).strip()\n",
    "\n",
    "def must_mention(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\n",
    "    required = reference_outputs.get(\"must_mention\") or []\n",
    "    out = _normalize(outputs.get(\"answer\", \"\"))\n",
    "    hits = 0\n",
    "    for phrase in required:\n",
    "        if _normalize(phrase) in out:\n",
    "            hits += 1\n",
    "    return hits / max(1, len(required))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d5ea8d",
   "metadata": {},
   "source": [
    "### Task 3: Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = client.evaluate(\n",
    "    agent_chain_with_formatting,\n",
    "    data=dataset.name,\n",
    "    evaluators=[correctness_evaluator, must_mention],\n",
    "    experiment_prefix=\"simple_agent, baseline\",\n",
    "    description=\"Testing the baseline system.\",\n",
    "    max_concurrency=4,\n",
    ")\n",
    "\n",
    "print(\"If running in a traced environment, open LangSmith to view comparison for the latest experiment.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5811c",
   "metadata": {},
   "source": [
    "## Part 2: LangGraph with Helpfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8662c4e",
   "metadata": {},
   "source": [
    "\n",
    "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
    "\n",
    "We add a conditional that either (a) executes tools, (b) ends if the answer is helpful, or (c) loops back for another refinement. We also impose a hard cap on turns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d813033",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** We instantiate a new `StateGraph(AgentState)` and add two nodes: `agent` (calls the LLM) and `action` (executes tool calls). We'll add a helpfulness gate next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efcf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_with_helpfulness_check = StateGraph(AgentState)\n",
    "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
    "graph_with_helpfulness_check.add_node(\"action\", tool_node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a9950",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** Set `\"agent\"` as the entry point so every run starts with the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad92df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_with_helpfulness_check.set_entry_point(\"agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3f740",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** `tool_call_or_helpful` routes to `action` if the model emitted `tool_calls`. Otherwise it runs a **helpfulness** check comparing the initial query and current final response. If helpful ‚Üí end; else ‚Üí continue. A hard cap prevents infinite loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def tool_call_or_helpful(state: AgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if getattr(last_message, \"tool_calls\", None):\n",
    "        return \"action\"\n",
    "\n",
    "    # hard loop cap\n",
    "    if len(state[\"messages\"]) > 10:\n",
    "        return \"end\"  # use mapping below\n",
    "\n",
    "    initial_query = state[\"messages\"][0]\n",
    "    final_response = state[\"messages\"][-1]\n",
    "\n",
    "    prompt_template = \"\"\"Given an initial query and a final response, determine if the final response is extremely helpful or not.\n",
    "Reply with 'Y' if helpful, 'N' if not.\n",
    "\n",
    "Initial Query:\n",
    "{initial_query}\n",
    "\n",
    "Final Response:\n",
    "{final_response}\n",
    "\"\"\"\n",
    "    helpfulness_prompt = PromptTemplate.from_template(prompt_template)\n",
    "    helpfulness_check_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "    helpfulness_chain = helpfulness_prompt | helpfulness_check_model | StrOutputParser()\n",
    "    helpfulness_response = helpfulness_chain.invoke({\n",
    "        \"initial_query\": initial_query.content,\n",
    "        \"final_response\": final_response.content\n",
    "    })\n",
    "    return \"end\" if \"Y\" in helpfulness_response else \"continue\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d9982",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** Connect conditional outcomes: `\"action\"` ‚Üí `action`, `\"continue\"` ‚Üí loop back to `agent`, `\"end\"` ‚Üí finish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "graph_with_helpfulness_check.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tool_call_or_helpful,\n",
    "    {\n",
    "        \"continue\": \"agent\",\n",
    "        \"action\": \"action\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13af36",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** Tool results always flow back to `agent` so the model can read them and decide next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49740f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f246e",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:** Compile to a runnable graph and test with a prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"What are Deep Research Agents?\")]}\n",
    "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: {node}\")\n",
    "        print(values[\"messages\"])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf8424",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: LangGraph for the \"Patterns\" of GenAI\n",
    "\n",
    "### Task 4: Helpfulness Check of GenAI Pattern Descriptions\n",
    "Ask the system about the 3 main patterns: Context Engineering, Fine-tuning, and Agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36172055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patterns = [\"Context Engineering\", \"Fine-tuning\", \"LLM-based agents\"]\n",
    "\n",
    "for pattern in patterns:\n",
    "    q = f\"What is {pattern} and when did it break onto the scene?\"\n",
    "    inputs = {\"messages\": [HumanMessage(content=q)]}\n",
    "    state = agent_with_helpfulness_check.invoke(inputs)\n",
    "    print(state[\"messages\"][-1].content)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e9184",
   "metadata": {},
   "source": [
    "\n",
    "#### ‚ùì Question #4: How could we improve the `must_mention` metric?\n",
    "\n",
    "**Answer (ideas):**\n",
    "- Normalize case/whitespace/punctuation before matching.\n",
    "- Allow synonyms/aliases (e.g., \"ZJU\" ‚âà \"Zhejiang University\").\n",
    "- Partial credit instead of all-or-nothing; token-level precision/recall.\n",
    "- Handle formatting (bullets/newlines).\n",
    "- Optionally require cited evidence (URLs/domains) when appropriate.\n",
    "- Penalize hallucinated entities that are *not* in the references.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
