{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Workflows + RAG — LangGraph (Session 6)\n",
    "\n",
    "This notebook builds a **multi-agentic LangGraph** application with two teams (Research + Document Writing) and a meta-supervisor. It includes fixes for:\n",
    "- Tavily import deprecation (`langchain_tavily`)\n",
    "- `read_document` start/end bug\n",
    "- RAG context serialization (convert `Document` objects to text)\n",
    "- `reference_previous_responses` returns text instead of raw Document repr\n",
    "- Clean streaming print helpers\n",
    "\n",
    "You will need **OpenAI** and **Tavily** API keys.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Dependencies & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass, nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
    "os.environ['TAVILY_API_KEY'] = getpass.getpass('TAVILY_API_KEY:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧱 Task 1 — Simple LangGraph RAG\n",
    "We load a local PDF (in `data/`) and build a Qdrant-in-memory vector store, then a simple 2-node graph: `retrieve -> generate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing import TypedDict, List\n",
    "from typing_extensions import Annotated\n",
    "import tiktoken\n",
    "import operator\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def tiktoken_len(text: str) -> int:\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model('gpt-4o')\n",
    "    except Exception:\n",
    "        enc = tiktoken.get_encoding('cl100k_base')\n",
    "    return len(enc.encode(text))\n",
    "directory_loader = DirectoryLoader('data', glob='**/*.pdf', loader_cls=PyMuPDFLoader)\n",
    "how_people_use_ai_documents = directory_loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=0, length_function=tiktoken_len)\n",
    "how_people_use_ai_chunks = text_splitter.split_documents(how_people_use_ai_documents)\n",
    "len(how_people_use_ai_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "qdrant_vectorstore = Qdrant.from_documents(documents=how_people_use_ai_chunks, embedding=embedding_model, location=':memory:')\n",
    "qdrant_retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_TEMPLATE = \"\"\"\n",
    "# CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "Use the provided context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context respond with \"I don't know\".\n",
    "\"\"\"\n",
    "chat_prompt = ChatPromptTemplate.from_messages([('human', HUMAN_TEMPLATE)])\n",
    "generator_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "def _format_docs(docs: List[Document]) -> str:\n",
    "    return '\\n\\n'.join([d.page_content for d in docs])\n",
    "def retrieve(state: RAGState):\n",
    "    return {'context': qdrant_retriever.invoke(state['question'])}\n",
    "def generate(state: RAGState):\n",
    "    generator_chain = chat_prompt | generator_llm | StrOutputParser()\n",
    "    response = generator_chain.invoke({'query': state['question'], 'context': _format_docs(state['context'])})\n",
    "    return {'response': response}\n",
    "rag_graph = StateGraph(RAGState).add_sequence([retrieve, generate])\n",
    "rag_graph.add_edge(START, 'retrieve')\n",
    "compiled_rag_graph = rag_graph.compile()\n",
    "compiled_rag_graph.invoke({'question': 'How does the average person use AI?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Task 2 — Helper Functions for Agent Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import functools\n",
    "import operator\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END\n",
    "from langchain_openai import ChatOpenAI\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {'messages': [HumanMessage(content=result['output'], name=name)]}\n",
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str) -> AgentExecutor:\n",
    "    system_prompt += ('\\nWork autonomously according to your specialty, using the tools available to you.'\n",
    "                      ' Do not ask for clarification.'\n",
    "                      ' Your other team members (and other teams) will collaborate with you with their own specialties.'\n",
    "                      ' You are chosen for a reason!')\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_prompt),\n",
    "        MessagesPlaceholder(variable_name='messages'),\n",
    "        MessagesPlaceholder(variable_name='agent_scratchpad'),\n",
    "    ])\n",
    "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "    return AgentExecutor(agent=agent, tools=tools)\n",
    "def create_team_supervisor(llm: ChatOpenAI, system_prompt: str, members: list):\n",
    "    options = ['FINISH'] + members\n",
    "    function_def = {'name':'route','description':'Select the next role.','parameters':{'title':'routeSchema','type':'object','properties':{'next':{'title':'Next','anyOf':[{'enum':options}],}},'required':['next'],}}\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_prompt),\n",
    "        MessagesPlaceholder(variable_name='messages'),\n",
    "        ('system','Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'),\n",
    "    ]).partial(options=str(options), team_members=', '.join(members))\n",
    "    return prompt | llm.bind_functions(functions=[function_def], function_call='route') | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔎 Task 3 — Research Team (Search + RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional\n",
    "from langchain_core.tools import tool\n",
    "from langchain_tavily import TavilySearch\n",
    "# Tavily tool will be initialized when API key is available\n",
    "tavily_tool = None\n",
    "try:\n",
    "    tavily_tool = TavilySearch(max_results=5)\n",
    "except Exception as e:\n",
    "    print(f\"Tavily tool initialization failed (API key required): {e}\")\n",
    "    # Create a mock tool for testing\n",
    "    from langchain_core.tools import tool\n",
    "    @tool\n",
    "    def mock_tavily_search(query: str) -> str:\n",
    "        \"\"\"Mock Tavily search tool for testing.\"\"\"\n",
    "        return f\"Mock search results for: {query}\"\n",
    "    tavily_tool = mock_tavily_search\n",
    "@tool\n",
    "def retrieve_information(query: Annotated[str, 'query to ask the retrieve information tool']):\n",
    "    \"\"\"Use Retrieval Augmented Generation to retrieve information about how people use AI\"\"\"\n",
    "    return compiled_rag_graph.invoke({'question': query})\n",
    "class ResearchTeamState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    team_members: List[str]\n",
    "    next: str\n",
    "research_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "search_agent = create_agent(research_llm, [tavily_tool], 'You are a research assistant who can search for up-to-date info using the tavily search engine.')\n",
    "search_node = functools.partial(agent_node, agent=search_agent, name='Search')\n",
    "research_agent = create_agent(research_llm, [retrieve_information], 'You are a research assistant who can provide specific information on how people use AI')\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name='HowPeopleUseAIRetriever')\n",
    "research_supervisor_agent = create_team_supervisor(\n",
    "    research_llm,\n",
    "    ('You are a supervisor tasked with managing a conversation between the following workers:  Search, HowPeopleUseAIRetriever. '\n",
    "     'Given the following user request, determine the subject to be researched and respond with the worker to act next. '\n",
    "     'Each worker will perform a task and respond with their results and status. '\n",
    "     'You should never ask your team to do anything beyond research. They are not required to write content or posts. '\n",
    "     'You should only pass tasks to workers that are specifically research focused. When finished, respond with FINISH.'),\n",
    "    ['Search','HowPeopleUseAIRetriever']\n",
    ")\n",
    "research_graph = StateGraph(ResearchTeamState)\n",
    "research_graph.add_node('Search', search_node)\n",
    "research_graph.add_node('HowPeopleUseAIRetriever', research_node)\n",
    "research_graph.add_node('ResearchSupervisor', research_supervisor_agent)\n",
    "research_graph.add_edge('Search','ResearchSupervisor')\n",
    "research_graph.add_edge('HowPeopleUseAIRetriever','ResearchSupervisor')\n",
    "research_graph.add_conditional_edges('ResearchSupervisor', lambda x: x['next'], {'Search':'Search','HowPeopleUseAIRetriever':'HowPeopleUseAIRetriever','FINISH':END})\n",
    "research_graph.set_entry_point('ResearchSupervisor')\n",
    "compiled_research_graph = research_graph.compile()\n",
    "def enter_research_chain(message: str):\n",
    "    return {'messages':[HumanMessage(content=message)]}\n",
    "research_chain = enter_research_chain | compiled_research_graph\n",
    "def pretty(s):\n",
    "    for k,v in s.items():\n",
    "        msgs = v.get('messages',[])\n",
    "        if msgs:\n",
    "            text = getattr(msgs[-1],'content','')[:200].replace('\\n',' ')\n",
    "            print(f'[{k}] {text}...')\n",
    "        else:\n",
    "            nxt = v.get('next')\n",
    "            if nxt:\n",
    "                print(f'[{k}] -> {nxt}')\n",
    "for s in research_chain.stream('How are people using AI to improve their lives?', {'recursion_limit':60}):\n",
    "    if '__end__' not in s:\n",
    "        pretty(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Question #1: Why a “powerful” LLM here?\n",
    "**A1.1**: Multi-step routing + tool choice under uncertainty (when to search vs use RAG; merging heterogeneous evidence) requires stronger planning and calibration.\n",
    "\n",
    "**A1.2**: The agent must decompose goals, decide tool order, judge sufficiency of evidence, and synthesize for a target audience—weak models loop or hallucinate here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✍️ Task 4 — Document Writing Team (Plan, Write, Edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Dict\n",
    "import uuid, os\n",
    "previous_cohort_loader = CSVLoader('data/AIE7_Projects_with_Domains.csv', content_columns=['Project Domain','Secondary Domain (if any)'])\n",
    "previous_cohort = previous_cohort_loader.load()\n",
    "qdrant_previous_cohort_vectorstore = Qdrant.from_documents(documents=previous_cohort, embedding=embedding_model, location=':memory:')\n",
    "qdrant_previous_cohort_retriever = qdrant_previous_cohort_vectorstore.as_retriever()\n",
    "os.makedirs('./content/data', exist_ok=True)\n",
    "def create_random_subdirectory():\n",
    "    random_id = str(uuid.uuid4())[:8]\n",
    "    p = os.path.join('./content/data', random_id)\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "WORKING_DIRECTORY = Path(create_random_subdirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional\n",
    "from langchain_core.tools import tool\n",
    "@tool\n",
    "def create_outline(points: Annotated[List[str],'List of main points or sections.'], file_name: Annotated[str,'File path to save the outline.']) -> Annotated[str,'Path of the saved outline file.']:\n",
    "    \"\"\"Create an outline file with the given points.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open('w') as f:\n",
    "        for i, pt in enumerate(points):\n",
    "            f.write(f\"{i+1}. {pt}\\n\")\n",
    "    return f'Outline saved to {file_name}'\n",
    "@tool\n",
    "def read_document(file_name: Annotated[str,'File path to read.'], start: Annotated[Optional[int],'1-indexed start line (default 1)']=1, end: Annotated[Optional[int],'1-indexed inclusive end line (default None)']=None) -> str:\n",
    "    \"\"\"Read a document file with optional line range.\"\"\"\n",
    "    p = (WORKING_DIRECTORY / file_name)\n",
    "    with p.open('r') as f:\n",
    "        lines = f.readlines()\n",
    "    s = max(1, (start or 1)) - 1\n",
    "    e = None if end is None else max(1, end)\n",
    "    return ''.join(lines[s:e])\n",
    "@tool\n",
    "def write_document(content: Annotated[str,'Text content to be written into the document.'], file_name: Annotated[str,'File path to save the document.']) -> Annotated[str,'Path of the saved document file.']:\n",
    "    \"\"\"Write content to a document file.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open('w') as f:\n",
    "        f.write(content)\n",
    "    return f'Document saved to {file_name}'\n",
    "@tool\n",
    "def reference_previous_responses(query: Annotated[str,'The query to search for in the previous responses.']) -> Annotated[str,'The previous responses that match the query.']:\n",
    "    \"\"\"Search for previous responses using the query.\"\"\"\n",
    "    docs = qdrant_previous_cohort_retriever.invoke(query)\n",
    "    return '\\n\\n'.join([d.page_content for d in docs])\n",
    "@tool\n",
    "def edit_document(file_name: Annotated[str,'Path of the document to be edited.'], inserts: Annotated[Dict[int,str],'{line_number: text} using 1-indexed lines'] = {}) -> Annotated[str,'Path of the edited document file.']:\n",
    "    \"\"\"Edit a document by inserting text at specified line numbers.\"\"\"\n",
    "    p = (WORKING_DIRECTORY / file_name)\n",
    "    with p.open('r') as f:\n",
    "        lines = f.readlines()\n",
    "    for ln, txt in sorted(inserts.items()):\n",
    "        if 1 <= ln <= len(lines)+1:\n",
    "            lines.insert(ln-1, txt+'\\n')\n",
    "        else:\n",
    "            return f'Error: Line number {ln} is out of range.'\n",
    "    with p.open('w') as f:\n",
    "        f.writelines(lines)\n",
    "    return f'Document edited and saved to {file_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Writing State, Prelude, and Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocWritingState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    team_members: str\n",
    "    next: str\n",
    "    current_files: str\n",
    "def prelude(state):\n",
    "    written_files = []\n",
    "    if not WORKING_DIRECTORY.exists():\n",
    "        WORKING_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        written_files = [f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.rglob('*')]\n",
    "    except Exception:\n",
    "        pass\n",
    "    if not written_files:\n",
    "        return {**state, 'current_files': 'No files written.'}\n",
    "    return {**state, 'current_files': '\\nBelow are files your team has written to the directory:\\n' + '\\n'.join([f' - {f}' for f in written_files])}\n",
    "authoring_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "doc_writer_agent = create_agent(authoring_llm, [write_document, edit_document, read_document], 'You are an expert writing customer assistance responses.\\nBelow are files currently in your directory:\\n{current_files}')\n",
    "context_aware_doc_writer_agent = prelude | doc_writer_agent\n",
    "doc_writing_node = functools.partial(agent_node, agent=context_aware_doc_writer_agent, name='DocWriter')\n",
    "note_taking_agent = create_agent(authoring_llm, [create_outline, read_document, reference_previous_responses], 'You are an expert senior researcher tasked with writing a customer assistance outline and taking notes to craft a customer assistance response.\\n{current_files}')\n",
    "context_aware_note_taking_agent = prelude | note_taking_agent\n",
    "note_taking_node = functools.partial(agent_node, agent=context_aware_note_taking_agent, name='NoteTaker')\n",
    "copy_editor_agent = create_agent(authoring_llm, [write_document, edit_document, read_document], 'You are an expert copy editor who focuses on fixing grammar, spelling, and tone issues\\nBelow are files currently in your directory:\\n{current_files}')\n",
    "context_aware_copy_editor_agent = prelude | copy_editor_agent\n",
    "copy_editing_node = functools.partial(agent_node, agent=context_aware_copy_editor_agent, name='CopyEditor')\n",
    "authoring_supervisor_agent = create_team_supervisor(authoring_llm, 'You are a supervisor tasked with managing a conversation between the following workers: {team_members}. You should always verify the technical contents after any edits are made. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When each team is finished, you must respond with FINISH.', ['DocWriter','NoteTaker','CopyEditor'])\n",
    "from langgraph.graph import StateGraph\n",
    "authoring_graph = StateGraph(DocWritingState)\n",
    "authoring_graph.add_node('DocWriter', doc_writing_node)\n",
    "authoring_graph.add_node('NoteTaker', note_taking_node)\n",
    "authoring_graph.add_node('CopyEditor', copy_editing_node)\n",
    "authoring_graph.add_node('AuthoringSupervisor', authoring_supervisor_agent)\n",
    "authoring_graph.add_edge('DocWriter','AuthoringSupervisor')\n",
    "authoring_graph.add_edge('NoteTaker','AuthoringSupervisor')\n",
    "authoring_graph.add_edge('CopyEditor','AuthoringSupervisor')\n",
    "authoring_graph.add_conditional_edges('AuthoringSupervisor', lambda x: x['next'], {'DocWriter':'DocWriter','NoteTaker':'NoteTaker','CopyEditor':'CopyEditor','FINISH':END})\n",
    "authoring_graph.set_entry_point('AuthoringSupervisor')\n",
    "compiled_authoring_graph = authoring_graph.compile()\n",
    "def enter_authoring_chain(message: str, members: List[str]):\n",
    "    return {'messages':[HumanMessage(content=message)], 'team_members': ', '.join(members)}\n",
    "authoring_chain = functools.partial(enter_authoring_chain, members=authoring_graph.nodes) | compiled_authoring_graph\n",
    "for s in authoring_chain.stream('What are the most common use-cases in this data. What are the most common domains?', {'recursion_limit':80}):\n",
    "    if '__end__' not in s:\n",
    "        pretty(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 Q2: How to force specific tools / flows?\n",
    "- Constrain tools per agent (each agent only sees the tools it should use).\n",
    "- Use a supervisor with a routing schema that only lists allowed next steps.\n",
    "- Insert guard nodes (e.g., always call RAG after Search before writing).\n",
    "- Add checks (e.g., if no citations, route back to Search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🕸️ Task 5 — Meta-Supervisor (Full Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "super_supervisor_agent = create_team_supervisor(super_llm, 'You are a supervisor tasked with managing a conversation between the following teams: {team_members}. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When all workers are finished, you must respond with FINISH.', ['Research team','Response team'])\n",
    "class SuperState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    next: str\n",
    "def get_last_message(state: SuperState) -> str:\n",
    "    return state['messages'][-1].content\n",
    "def join_graph(response: dict):\n",
    "    return {'messages':[response['messages'][-1]]}\n",
    "from langgraph.graph import StateGraph\n",
    "super_graph = StateGraph(SuperState)\n",
    "super_graph.add_node('Research team', get_last_message | research_chain | join_graph)\n",
    "super_graph.add_node('Response team', get_last_message | authoring_chain | join_graph)\n",
    "super_graph.add_node('SuperSupervisor', super_supervisor_agent)\n",
    "super_graph.add_edge('Research team','SuperSupervisor')\n",
    "super_graph.add_edge('Response team','SuperSupervisor')\n",
    "super_graph.add_conditional_edges('SuperSupervisor', lambda x: x['next'], {'Response team':'Response team','Research team':'Research team','FINISH':END})\n",
    "super_graph.set_entry_point('SuperSupervisor')\n",
    "compiled_super_graph = super_graph.compile()\n",
    "from pathlib import Path\n",
    "def create_random_subdirectory():\n",
    "    import uuid, os\n",
    "    p = Path('./content/data')/str(uuid.uuid4())[:8]\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "WORKING_DIRECTORY = Path(create_random_subdirectory())\n",
    "for s in compiled_super_graph.stream({'messages':[HumanMessage(content='Write a report on the rise of context engineering in the LLM Space in 2025, and how it\\'s impacting how people are using AI.')]} , {'recursion_limit':40}):\n",
    "    if '__end__' not in s:\n",
    "        pretty(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Notes\n",
    "- Ensure you have `data/` present with the referenced PDF and CSV files.\n",
    "- For reproducibility, pin versions in `requirements.txt` as suggested in the instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
